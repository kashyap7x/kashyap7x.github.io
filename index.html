<!DOCTYPE html>
<html>
<head>
<title>Kashyap Chitta | Machine Learning Researcher</title>
<link rel="stylesheet" type="text/css" href="style.css">

<script type="text/javascript" src="js/hidebib.js"></script>

</head>
<body>

<div class="section">
<h1>Kashyap Chitta</h1>
</div>
<hr>

<div class="section">
<table>
  <tr valign="top"> <td style="width: 700px; vertical-align: top;">
  I am a PhD student at the Max Planck Institute for Intelligent Systems in T&uumlbingen, Germany, where I am part of the <a href = "https://avg.is.tuebingen.mpg.de">Autonomous Vision Group</a> led by <a href = "http://www.cvlibs.net">Prof. Andreas Geiger</a>. My research is in machine learning and computer vision. I am interested in uncertainty estimation, with a focus on explaining errors and improving data efficiency for machine learning algorithms. Previously, I graduated with a Master's degree in Computer Vision from CMU, where I was advised by <a href = "http://www.cs.cmu.edu/~hebert/">Prof. Martial Hebert</a>. During this time, I was also a Deep Learning Intern on two occasions at <a href = "https://www.nvidia.com/en-us/deep-learning-ai/">NVIDIA</a>, Santa Clara working with <a href = "http://www.josemalvarez.net/">Dr. Jose M. Alvarez</a>.
  <p><br></p>
  <p>
    <a href="javascript:toggleblock('email')">email</a> | <a href="https://github.com/kashyap7x">github</a>  | <a href="https://www.linkedin.com/in/kchitta/">linkedin</a> | <a href="https://twitter.com/kashyap7x">twitter</a> | <a href="https://scholar.google.com/citations?user=vX5i2CcAAAAJ&hl=en">google scholar</a>
  </p>
  <pre xml:space="preserve" id="email" style="font-size: 12px">

firstname DOT lastname AT tue DOT mpg DOT de
  </pre>
  <script xml:space="preserve" language="JavaScript">
  hideblock('email');
  </script>
  </td>

  <td width="300"><img src="img.jpg" alt="My picture" height=240 align="right"/></td>
    </tr>
  </table>
</div>

<div class="section">
<h2> Publications </h2>
<!--------------------------------------------------------------------------->
<div class="paper" id="chitta2019training">
<img class="paper" title="Less is More: An Exploration of Data Redundancy with Active Dataset Subsampling" src="figures/chitta2019training.png" />
<p> <strong style="color:red">[New]</strong> <b id="papertitle">Training Data Distribution Search with Ensemble Active Learning</b> <br/>
<strong>Kashyap Chitta</strong>, Jose M. Alvarez, Elmar Haussmann, Clement Farabet <br/>
ArXiv e-prints, 2019 <br/>
<a href="https://arxiv.org/pdf/1905.12737.pdf">pdf </a>  &nbsp <a href="javascript:toggleblock('chitta2019trainingAbs')">abstract </a>  &nbsp <a href="javascript:toggleblock('chitta2019trainingBib')">bibtex </a> </p>
<div class="papermeta" id="chitta2019trainingMeta">
<em id="chitta2019trainingAbs">Deep Neural Networks (DNNs) often rely on very large datasets for training. Given the large size of such datasets, it is conceivable that they contain certain samples that either do not contribute or negatively impact the DNN's optimization. Modifying the training distribution in a way that excludes such samples could provide an effective solution to both improve performance and reduce training time. In this paper, we propose to scale up ensemble Active Learning methods to perform acquisition at a large scale (10k to 500k samples at a time). We do this with ensembles of hundreds of models, obtained at a minimal computational cost by reusing intermediate training checkpoints. This allows us to automatically and efficiently perform a training data distribution search for large labeled datasets. We observe that our approach obtains favorable subsets of training data, which can be used to train more accurate DNNs than training with the entire dataset. We perform an extensive experimental study of this phenomenon on three image classification benchmarks (CIFAR-10, CIFAR-100 and ImageNet), analyzing the impact of initialization schemes, acquisition functions and ensemble configurations. We demonstrate that data subsets identified with a lightweight ResNet-18 ensemble remain effective when used to train deep models like ResNet-101 and DenseNet-121. Our results provide strong empirical evidence that optimizing the training data distribution can provide significant benefits on large scale vision tasks. </em>
<pre xml:space="preserve" id="chitta2019trainingBib">

@inProceedings{chitta2019training,
  title={Training Data Distribution Search with Ensemble Active Learning},
  author = {Kashyap Chitta
  and Jose M. Alvarez
  and Elmar Haussmann
  and Clement Farabet},
  booktitle={ArXiv e-prints},
  year={2019}
}</pre></td>
<script language="javascript" type="text/javascript" xml:space="preserve">
hideblock('chitta2019trainingAbs');
hideblock('chitta2019trainingBib');
</script>
</div>
</div>
<!--------------------------------------------------------------------------->

<!--------------------------------------------------------------------------->
<div class="paper" id="chitta2020quadtree">
<img class="paper" title="Quadtree Generating Networks: Efficient Hierarchical Scene Parsing with Sparse Convolutions" src="figures/chitta2020quadtree.png" />
<p> <strong style="color:red">[New]</strong> <b id="papertitle">Quadtree Generating Networks: Efficient Hierarchical Scene Parsing with Sparse Convolutions</b> <br/>
<strong>Kashyap Chitta</strong>, Jose M. Alvarez, Martial Hebert <br/>
IEEE Winter Conference on Applications of Computer Vision (WACV), 2020 <br/>
<a href="https://arxiv.org/pdf/1907.11821.pdf">pdf </a>  &nbsp <a href="javascript:toggleblock('chitta2020quadtreeAbs')">abstract </a>  &nbsp <a href="javascript:toggleblock('chitta2020quadtreeBib')">bibtex </a>  &nbsp <a href="https://github.com/kashyap7x/QGN">code </a> </p>
<div class="papermeta" id="chitta2020quadtreeMeta">
<em id="chitta2020quadtreeAbs">Semantic segmentation with Convolutional Neural Networks is a memory-intensive task due to the high spatial resolution of feature maps and output predictions. In this paper, we present Quadtree Generating Networks (QGNs), a novel approach able to drastically reduce the memory footprint of modern semantic segmentation networks. The key idea is to use quadtrees to represent the predictions and target segmentation masks instead of dense pixel grids. Our quadtree representation enables hierarchical processing of an input image, with the most computationally demanding layers only being used at regions in the image containing boundaries between classes. In addition, given a trained model, our representation enables flexible inference schemes to trade-off accuracy and computational cost, allowing the network to adapt in constrained situations such as embedded devices. We demonstrate the benefits of our approach on the Cityscapes, SUN-RGBD and ADE20k datasets. On Cityscapes, we obtain an relative 3% mIoU improvement compared to a dilated network with similar memory consumption; and only receive a 3% relative mIoU drop compared to a large dilated network, while reducing memory consumption by over 4&times. </em>
<pre xml:space="preserve" id="chitta2020quadtreeBib">

@inProceedings{chitta2020quadtree,
  title={Quadtree Generating Networks: Efficient Hierarchical Scene Parsing with Sparse Convolutions},
  author = {Kashyap Chitta
  and Jose M. Alvarez
  and Martial Hebert},
  booktitle={IEEE Winter Conference on Applications of Computer Vision (WACV)},
  year={2020}
}</pre></td>
<script language="javascript" type="text/javascript" xml:space="preserve">
hideblock('chitta2020quadtreeAbs');
hideblock('chitta2020quadtreeBib');
</script>
</div>
</div>
<!--------------------------------------------------------------------------->

<!--------------------------------------------------------------------------->
<div class="paper" id="chitta2018largescale">
<img class="paper" title="Large-Scale Visual Active Learning with Deep Probabilistic Ensembles" src="figures/chitta2018largescale.png" />
<p> <b id="papertitle">Large-Scale Visual Active Learning with Deep Probabilistic Ensembles</b> <br/>
<strong>Kashyap Chitta</strong>, Jose M. Alvarez, Adam Lesnikowski <br/>
ArXiv e-prints, 2018 <br/>
<a href="https://arxiv.org/pdf/1811.03575.pdf">pdf </a>  &nbsp <a href="javascript:toggleblock('chitta2018largescaleAbs')">abstract </a>  &nbsp <a href="javascript:toggleblock('chitta2018largescaleBib')">bibtex </a> </p>
<div class="papermeta" id="chitta2018largescaleMeta">
<em id="chitta2018largescaleAbs">Annotating the right data for training deep neural networks is an important challenge. Active learning using uncertainty estimates from Bayesian Neural Networks (BNNs) could provide an effective solution to this. Despite being theoretically principled, BNNs require approximations to be applied to large-scale problems, where both performance and uncertainty estimation are crucial. In this paper, we introduce Deep Probabilistic Ensembles (DPEs), a scalable technique that uses a regularized ensemble to approximate a deep BNN. We conduct a series of large-scale visual active learning experiments to evaluate DPEs on classification with the CIFAR-10, CIFAR-100 and ImageNet datasets, and semantic segmentation with the BDD100k dataset. Our models require significantly less training data to achieve competitive performances, and steadily improve upon strong active learning baselines as the annotation budget is increased. </em>
<pre xml:space="preserve" id="chitta2018largescaleBib">

@inProceedings{chitta2018largescale,
  title={Large-Scale Visual Active Learning with Deep Probabilistic Ensembles},
  author = {Kashyap Chitta
  and Jose M. Alvarez
  and Adam Lesnikowski},
  booktitle={ArXiv e-prints},
  year={2018}
}</pre></td>
<script language="javascript" type="text/javascript" xml:space="preserve">
hideblock('chitta2018largescaleAbs');
hideblock('chitta2018largescaleBib');
</script>
</div>
</div>
<!--------------------------------------------------------------------------->

<!--------------------------------------------------------------------------->
<div class="paper" id="chitta2018adaptive">
<img class="paper" title="Adaptive Semantic Segmentation with a Strategic Curriculum of Proxy Labels" src="figures/chitta2018adaptive.png" />
<p> <b id="papertitle">Adaptive Semantic Segmentation with a Strategic Curriculum of Proxy Labels</b> <br/>
<strong>Kashyap Chitta</strong>, Jianwei Feng, Martial Hebert <br/>
ArXiv e-prints, 2018 <br/>
<a href="https://arxiv.org/pdf/1811.03542.pdf">pdf </a>  &nbsp <a href="javascript:toggleblock('chitta2018adaptiveAbs')">abstract </a>  &nbsp <a href="javascript:toggleblock('chitta2018adaptiveBib')">bibtex </a>  &nbsp <a href="https://github.com/kashyap7x/Domain-Adapatation">code </a> </p>
<div class="papermeta" id="chitta2018adaptiveMeta">
<em id="chitta2018adaptiveAbs">Training deep networks for semantic segmentation requires annotation of large amounts of data, which can be time-consuming and expensive. Unfortunately, these trained networks still generalize poorly when tested in domains not consistent with the training data. In this paper, we show that by carefully presenting a mixture of labeled source domain and proxy-labeled target domain data to a network, we can achieve state-of-the-art unsupervised domain adaptation results. With our design, the network progressively learns features specific to the target domain using annotation from only the source domain. We generate proxy labels for the target domain using the network's own predictions. Our architecture then allows selective mining of easy samples from this set of proxy labels, and hard samples from the annotated source domain. We conduct a series of experiments with the GTA5, Cityscapes and BDD100k datasets on synthetic-to-real domain adaptation and geographic domain adaptation, showing the advantages of our method over baselines and existing approaches.</em>
<pre xml:space="preserve" id="chitta2018adaptiveBib">

@inProceedings{chitta2018adaptive,
  title={Adaptive Semantic Segmentation with a Strategic Curriculum of Proxy Labels},
  author = {Kashyap Chitta
  and Jianwei Feng
  and Martial Hebert},
  booktitle={ArXiv e-prints},
  year={2018}
}</pre></td>
<script language="javascript" type="text/javascript" xml:space="preserve">
hideblock('chitta2018adaptiveAbs');
hideblock('chitta2018adaptiveBib');
</script>
</div>
</div>
<!--------------------------------------------------------------------------->

<!--------------------------------------------------------------------------->
<div class="paper" id="chitta2018deep">
<img class="paper" title="Deep Probabilistic Ensembles: Approximate Variational Inference through KL Regularization" src="figures/chitta2018deep.png" />
<p><b id="papertitle">Deep Probabilistic Ensembles: Approximate Variational Inference through KL Regularization</b> <br/>
<strong>Kashyap Chitta</strong>, Jose M. Alvarez, Adam Lesnikowski <br/>
Workshop on Bayesian Deep Learning (BDL), NeurIPS, 2018 <br/>
<a href="https://arxiv.org/pdf/1811.02640.pdf">pdf </a>  &nbsp <a href="javascript:toggleblock('chitta2018deepAbs')">abstract </a>  &nbsp <a href="javascript:toggleblock('chitta2018deepBib')">bibtex </a> </p>
<div class="papermeta" id="chitta2018deepMeta">
<em id="chitta2018deepAbs"> In this paper, we introduce Deep Probabilistic Ensembles (DPEs), a scalable technique that uses a regularized ensemble to approximate a deep Bayesian Neural Network (BNN). We do so by incorporating a KL divergence penalty term into the training objective of an ensemble, derived from the evidence lower bound used in variational inference. We evaluate the uncertainty estimates obtained from our models for active learning on visual classification. Our approach steadily improves upon active learning baselines as the annotation budget is increased.</em>
<pre xml:space="preserve" id="chitta2018deepBib">

@inProceedings{chitta2018deep,
  title={Deep Probabilistic Ensembles: Approximate Variational Inference through KL Regularization},
  author = {Kashyap Chitta
  and Jose M. Alvarez
  and Adam Lesnikowski},
  booktitle={Workshop on Bayesian Deep Learning (BDL), Conference on Neural Information Processing Systems (NeurIPS)},
  year={2018}
}</pre></td>
<script language="javascript" type="text/javascript" xml:space="preserve">
hideblock('chitta2018deepAbs');
hideblock('chitta2018deepBib');
</script>
</div>
</div>
<!--------------------------------------------------------------------------->

<!--------------------------------------------------------------------------->
<div class="paper" id="chitta2018targeted">
<img class="paper" title="Targeted Kernel Networks: Faster Convolutions with Attentive Regularization" src="figures/chitta2018targeted.png" />
<p><b id="papertitle">Targeted Kernel Networks: Faster Convolutions with Attentive Regularization</b> <br/>
<strong>Kashyap Chitta</strong><br/>
Workshop on Compact and Efficient Feature Representation and Learning in Computer Vision (CEFRL), ECCV, 2018 <br/>
<a href="https://arxiv.org/pdf/1806.00523.pdf">pdf </a>  &nbsp <a href="javascript:toggleblock('chitta2018targetedAbs')">abstract </a>  &nbsp <a href="javascript:toggleblock('chitta2018targetedBib')">bibtex </a>  &nbsp <a href="https://github.com/kashyap7x/Attentive-Regularization">code </a> </p>
<div class="papermeta" id="chitta2018targetedMeta">
<em id="chitta2018targetedAbs"> We propose Attentive Regularization (AR), a method to constrain the activation maps of kernels in Convolutional Neural Networks (CNNs) to specific regions of interest (ROIs). Each kernel learns a location of specialization along with its weights through standard backpropagation. A differentiable attention mechanism requiring no additional supervision is used to optimize the ROIs. Traditional CNNs of different types and structures can be modified with this idea into equivalent Targeted Kernel Networks (TKNs), while keeping the network size nearly identical. By restricting kernel ROIs, we reduce the number of sliding convolutional operations performed throughout the network in its forward pass, speeding up both training and inference. We evaluate our proposed architecture on both synthetic and natural tasks across multiple domains. TKNs obtain significant improvements over baselines, requiring less computation (around an order of magnitude) while achieving superior performance.</em>
<pre xml:space="preserve" id="chitta2018targetedBib">

@inProceedings{chitta2018targeted,
  title={Targeted Kernel Networks: Faster Convolutions with Attentive Regularization},
  author = {Kashyap Chitta},
  booktitle={Workshop on Compact and Efficient Feature Representation and Learning in Computer Vision (CEFRL), European Conference on Computer Vision (ECCV)},
  year={2018}
}</pre></td>
<script language="javascript" type="text/javascript" xml:space="preserve">
hideblock('chitta2018targetedAbs');
hideblock('chitta2018targetedBib');
</script>
</div>
</div>
<!--------------------------------------------------------------------------->

<!--------------------------------------------------------------------------->
<div class="paper" id="patel2018learning">
<img class="paper" title="Learning Sampling Policies for Domain Adaptation" src="figures/patel2018learning.png" />
<p><b id="papertitle">Learning Sampling Policies for Domain Adaptation</b> <br/>
Yash Patel*, <strong>Kashyap Chitta</strong>*, Bhavan Jasani* <br/>
ArXiv e-prints, 2018 <br/>
<a href="https://arxiv.org/pdf/1805.07641.pdf">pdf </a>  &nbsp <a href="javascript:toggleblock('patel2018learningAbs')">abstract </a>  &nbsp <a href="javascript:toggleblock('patel2018learningBib')">bibtex </a>  &nbsp <a href="https://github.com/yash0307/LearningSamplingPolicies">code </a> </p>
<div class="papermeta" id="patel2018learningMeta">
<em id="patel2018learningAbs">We address the problem of semi-supervised domain adaptation of classification algorithms through deep Q-learning. The core idea is to consider the predictions of a source domain network on target domain data as noisy labels, and learn a policy to sample from this data so as to maximize classification accuracy on a small annotated reward partition of the target domain. Our experiments show that learned sampling policies construct labeled sets that improve accuracies of visual classifiers over baselines.</em>
<pre xml:space="preserve" id="patel2018learningBib">

@inProceedings{patel2018learning,
  title={Learning Sampling Policies for Domain Adaptation},
  author = {Yash Patel
  and Kashyap Chitta
  and Bhavan Jasani},
  booktitle={ArXiv e-prints},
  year={2018}
}</pre></td>
<script language="javascript" type="text/javascript" xml:space="preserve">
hideblock('patel2018learningAbs');
hideblock('patel2018learningBib');
</script>
</div>
</div>
<!--------------------------------------------------------------------------->

<!--------------------------------------------------------------------------->
<div class="paper" id="chitta2016reduced">
<img class="paper" title="A Reduced Region of Interest Based Approach for Facial Expression Recognition from Static Images" src="figures/chitta2016reduced.png" />
<p><b id="papertitle">A Reduced Region of Interest Based Approach for Facial Expression Recognition from Static Images</b> <br/>
<strong>Kashyap Chitta</strong>, Neeraj N. Sajjan <br/>
IEEE TENCON, 2016 <br/>
<a href="https://ieeexplore.ieee.org/document/7848553">pdf </a>  &nbsp <a href="javascript:toggleblock('chitta2016reducedAbs')">abstract </a>  &nbsp <a href="javascript:toggleblock('chitta2016reducedBib')">bibtex </a> </p>
<div class="papermeta" id="chitta2016reducedMeta">
<em id="chitta2016reducedAbs">The general approach to facial expression recognition involves three stages: face acquisition, feature extraction and expression recognition. A series of steps are used during feature extraction, and the robustness of a recognition model depends on the ability to handle exceptions over all these steps. This paper details experiments conducted to classify images by facial expression using reduced regions of interest and discriminative salient patches on the face, while minimizing the number of steps required for their localization. The performance of various feature descriptors is analyzed and a model for expression recognition for which experiments on the JAFFE database show effectiveness is proposed.</em>
<pre xml:space="preserve" id="chitta2016reducedBib">

@inProceedings{chitta2016reduced,
  title={A Reduced Region of Interest Based Approach for Facial Expression Recognition from Static Images},
  author = {Kashyap Chitta
  and Neeraj N. Sajjan},
  booktitle={IEEE Region-10 Conference (TENCON)},
  year={2016}
}</pre></td>
<script language="javascript" type="text/javascript" xml:space="preserve">
hideblock('chitta2016reducedAbs');
hideblock('chitta2016reducedBib');
</script>
</div>
</div>
<!--------------------------------------------------------------------------->

<!--------------------------------------------------------------------------->
<!--- TEMPLATE
<div class="paper" id="paperId">
  <img class="paper" title="X" src="images/X.png" />
  <p><b id="papertitle">Title</b> <br/>
  <strong>Kashyap Chitta</strong>, Author2, Author3 <br />
  Conference, 2019<br />
  <a href="link">pdf</a>  &nbsp <a href="page">project page</a>  &nbsp <a href="javascript:toggleblock('paperIdAbs')">abstract</a> &nbsp <a href="javascript:toggleblock('paperIdBib')">bibtex</a>  &nbsp <a href="codelink">code</a> </p>

  <div class="papermeta" id="paperIdMeta">
  <em id="paperIdAbs">ABSTRACT</em></p>
  <pre xml:space="preserve" id="paperIdBib" style="font-size: 12px">
@inProceedings{
BIBTEX
}</pre></td>
  <script language="javascript" type="text/javascript" xml:space="preserve">
     hideblock('paperIdAbs');
     hideblock('paperIdBib');
  </script>
  </div>
</div>
-->
<!--------------------------------------------------------------------------->

</body>
</html>
