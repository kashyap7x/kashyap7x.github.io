<!DOCTYPE html>
<html>
<head>
<title>Kashyap Chitta | AI Researcher</title>
<link rel="stylesheet" type="text/css" href="style.css">

<script type="text/javascript" src="js/hidebib.js"></script>

</head>
<body>

<div class="section">
<h1>Kashyap Chitta</h1>
</div>
<hr>

<div class="section">
<table>
  <tr valign="top"> <td style="width: 700px; vertical-align: top;">
  I am a PhD student at the Max Planck Institute for Intelligent Systems and the University of T&uumlbingen, Germany, where I am part of the <a href = "https://uni-tuebingen.de/en/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/autonomous-vision/home/">Autonomous Vision Group</a> led by <a href = "http://www.cvlibs.net">Prof. Andreas Geiger</a>. My research is at the intersection of robotics, machine learning and computer vision. I am currently interested in scene representations for improving the robustness and generalization of learned vision-based control policies. Previously, I graduated with a Master's degree in Computer Vision from CMU, where I was advised by <a href = "http://www.cs.cmu.edu/~hebert/">Prof. Martial Hebert</a>. During this time, I was also a Deep Learning Intern on two occasions at NVIDIA working with <a href = "http://www.josemalvarez.net/">Dr. Jose M. Alvarez</a>.
  <p><br></p>
  <p>
    <a href="javascript:toggleblock('email')">email</a> | <a href="https://github.com/kashyap7x">github</a>  | <a href="https://www.linkedin.com/in/kchitta/">linkedin</a> | <a href="https://twitter.com/kashyap7x">twitter</a> | <a href="https://scholar.google.com/citations?user=vX5i2CcAAAAJ&hl=en">google scholar</a>
  </p>
  <pre xml:space="preserve" id="email" style="font-size: 12px">

firstname DOT lastname AT tue DOT mpg DOT de
  </pre>
  <script xml:space="preserve" language="JavaScript">
  hideblock('email');
  </script>
  </td>

  <td width="420"><img src="img.jpg" alt="My picture" height=270 align="right"/></td>
    </tr>
  </table>
</div>

<div class="section">
<h2> Publications </h2>

<!--------------------------------------------------------------------------->
<div class="paper" id="sauer2021projected">
  <img class="paper" title="Projected GANs Converge Faster" src="figures/sauer2021projected.png" />
  <p> <b id="papertitle"><a href="https://sites.google.com/view/projected-gan">Projected GANs Converge Faster</a></b> <br/>
  Axel Sauer, <strong>Kashyap Chitta</strong>, Jens Muller, Andreas Geiger <br/>
  Advances in Neural Information Processing Systems (NeurIPS), 2021 <br/>
  <a href="https://proceedings.neurips.cc/paper/2021/file/9219adc5c42107c4911e249155320648-Paper.pdf">pdf </a>  &nbsp <a href="javascript:toggleblock('sauer2021projectedAbs')">abstract </a>  &nbsp <a href="javascript:toggleblock('sauer2021projectedBib')">bibtex </a> &nbsp <a href="https://github.com/autonomousvision/projected_gan">code </a> </p>
  <div class="papermeta" id="sauer2021projectedMeta">
  <em id="sauer2021projectedAbs">Generative Adversarial Networks (GANs) produce high-quality images but are challenging to train. They need careful regularization, vast amounts of compute, and expensive hyper-parameter sweeps. We make significant headway on these issues by projecting generated and real samples into a fixed, pretrained feature space. Motivated by the finding that the discriminator cannot fully exploit features from deeper layers of the pretrained model, we propose a more effective strategy that mixes features across channels and resolutions. Our Projected GAN improves image quality, sample efficiency, and convergence speed. It is further compatible with resolutions of up to one Megapixel and advances the state-of-the-art Fr√©chet Inception Distance (FID) on twenty-two benchmark datasets. Importantly, Projected GANs match the previously lowest FIDs up to 40 times faster, cutting the wall-clock time from 5 days to less than 3 hours given the same computational resources.</em>
  <pre xml:space="preserve" id="sauer2021projectedBib">
  
  @inProceedings{sauer2021projected,
    title={Projected GANs Converge Faster},
    author = {Axel Sauer
      and Kashyap Chitta
      and Jens Muller
      and Andreas Geiger},
    booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
    year={2021}
  }</pre></td>
  <script language="javascript" type="text/javascript" xml:space="preserve">
  hideblock('sauer2021projectedAbs');
  hideblock('sauer2021projectedBib');
  </script>
  </div>
  </div>
<!--------------------------------------------------------------------------->

<!--------------------------------------------------------------------------->
<div class="paper" id="chitta2021neat">
<img class="paper" title="NEAT: Neural Attention Fields for End-to-End Autonomous Driving" src="figures/chitta2021neat.png" />
<p> <b id="papertitle"><a href="https://github.com/autonomousvision/neat">NEAT: Neural Attention Fields for End-to-End Autonomous Driving</a></b> <br/>
<strong>Kashyap Chitta*</strong>, Aditya Prakash*, Andreas Geiger <br/>
International Conference on Computer Vision (ICCV), 2021 <br/>
<a href="http://www.cvlibs.net/publications/Chitta2021ICCV.pdf">pdf </a>  &nbsp <a href="javascript:toggleblock('chitta2021neatAbs')">abstract </a>  &nbsp <a href="javascript:toggleblock('chitta2021neatBib')">bibtex </a>  &nbsp <a href="https://github.com/autonomousvision/neat">code </a> </p>
<div class="papermeta" id="chitta2021neatMeta">
<em id="chitta2021neatAbs">Efficient reasoning about the semantic, spatial, and temporal structure of a scene is a crucial prerequisite for autonomous driving. We present NEural ATtention fields (NEAT), a novel representation that enables such reasoning for end-to-end imitation learning models. NEAT is a continuous function which maps locations in Bird's Eye View (BEV) scene coordinates to waypoints and semantics, using intermediate attention maps to iteratively compress high-dimensional 2D image features into a compact representation. This allows our model to selectively attend to relevant regions in the input while ignoring information irrelevant to the driving task, effectively associating the images with the BEV representation. In a new evaluation setting involving adverse environmental conditions and challenging scenarios, NEAT outperforms several strong baselines and achieves driving scores on par with the privileged CARLA expert used to generate its training data. Furthermore, visualizing the attention maps for models with NEAT intermediate representations provides improved interpretability.</em>
<pre xml:space="preserve" id="chitta2021neatBib">

@inProceedings{chitta2021neat,
  title={NEAT: Neural Attention Fields for End-to-End Autonomous Driving},
  author = {Kashyap Chitta
    and Aditya Prakash
    and Andreas Geiger},
  booktitle={International Conference on Computer Vision (ICCV)},
  year={2021}
}</pre></td>
<script language="javascript" type="text/javascript" xml:space="preserve">
hideblock('chitta2021neatAbs');
hideblock('chitta2021neatBib');
</script>
</div>
</div>
<!--------------------------------------------------------------------------->

<!--------------------------------------------------------------------------->
<div class="paper" id="weis2021benchmarking">
  <img class="paper" title="Benchmarking Unsupervised Object Representations for Video Sequences" src="figures/weis2021benchmarking.png" />
  <p> <b id="papertitle"><a href="https://github.com/ecker-lab/object-centric-representation-benchmark">Benchmarking Unsupervised Object Representations for Video Sequences</a></b> <br/>
  Marissa A. Weis, <strong>Kashyap Chitta</strong>, Yash Sharma, Wieland Brendel, Matthias Bethge, Andreas Geiger, Alexander S. Ecker <br/>
  Journal of Machine Learning Research (JMLR), 2021 <br/>
  <a href="https://arxiv.org/pdf/2006.07034.pdf">pdf </a>  &nbsp <a href="javascript:toggleblock('weis2021benchmarkingAbs')">abstract </a>  &nbsp <a href="javascript:toggleblock('weis2021benchmarkingBib')">bibtex </a> &nbsp <a href="https://github.com/ecker-lab/object-centric-representation-benchmark">code </a> </p>
  <div class="papermeta" id="weis2021benchmarkingMeta">
  <em id="weis2021benchmarkingAbs">Perceiving the world in terms of objects and tracking them through time is a crucial prerequisite for reasoning and scene understanding. Recently, several methods have been proposed for unsupervised learning of object-centric representations. However, since these models were evaluated on different downstream tasks, it remains unclear how they compare in terms of basic perceptual abilities such as detection, figure-ground segmentation and tracking of objects. To close this gap, we design a benchmark with four data sets of varying complexity and seven additional test sets featuring challenging tracking scenarios relevant for natural videos. Using this benchmark, we compare the perceptual abilities of four object-centric approaches: ViMON, a video-extension of MONet, based on recurrent spatial attention, OP3, which exploits clustering via spatial mixture models, as well as TBA and SCALOR, which use explicit factorization via spatial transformers. Our results suggest that the architectures with unconstrained latent representations learn more powerful representations in terms of object detection, segmentation and tracking than the spatial transformer based architectures. We also observe that none of the methods are able to gracefully handle the most challenging tracking scenarios despite their synthetic nature, suggesting that our benchmark may provide fruitful guidance towards learning more robust object-centric video representations. </em>
  <pre xml:space="preserve" id="weis2021benchmarkingBib">
  
  @inProceedings{weis2021benchmarking,
    title={Benchmarking Unsupervised Object Representations for Video Sequences},
    author = {Marissa A. Weis
      and Kashyap Chitta
      and Yash Sharma
      and Wieland Brendel
      and Matthias Bethge
      and Andreas Geiger
      and Alexander S. Ecker},
    booktitle={Journal of Machine Learning Research (JMLR)},
    year={2021}
  }</pre></td>
  <script language="javascript" type="text/javascript" xml:space="preserve">
  hideblock('weis2021benchmarkingAbs');
  hideblock('weis2021benchmarkingBib');
  </script>
  </div>
  </div>
  <!--------------------------------------------------------------------------->

<!--------------------------------------------------------------------------->
<div class="paper" id="prakash2021multimodal">
<img class="paper" title="Multi-Modal Fusion Transformer for End-to-End Autonomous Driving" src="figures/prakash2021multimodal.png" />
<p> <b id="papertitle"><a href="https://github.com/autonomousvision/transfuser">Multi-Modal Fusion Transformer for End-to-End Autonomous Driving</a></b> <br/>
Aditya Prakash*, <strong>Kashyap Chitta*</strong>, Andreas Geiger <br/>
Conference on Computer Vision and Pattern Recognition (CVPR), 2021 <br/>
<a href="https://arxiv.org/pdf/2104.09224.pdf">pdf </a>  &nbsp <a href="javascript:toggleblock('prakash2021multimodalAbs')">abstract </a>  &nbsp <a href="javascript:toggleblock('prakash2021multimodalBib')">bibtex </a>  &nbsp <a href="https://github.com/autonomousvision/transfuser">code </a> &nbsp <a href="https://autonomousvision.github.io/transfuser/">blog </a> </p>
<div class="papermeta" id="prakash2021multimodalMeta">
<em id="prakash2021multimodalAbs">How should representations from complementary sensors be integrated for autonomous driving? Geometry-based sensor fusion has shown great promise for perception tasks such as object detection and motion forecasting. However, for the actual driving task, the global context of the 3D scene is key, e.g. a change in traffic light state can affect the behavior of a vehicle geometrically distant from that traffic light. Geometry alone may therefore be insufficient for effectively fusing representations in end-to-end driving models. In this work, we demonstrate that imitation learning policies based on existing sensor fusion methods under-perform in the presence of a high density of dynamic agents and complex scenarios, which require global contextual reasoning, such as handling traffic oncoming from multiple directions at uncontrolled intersections. Therefore, we propose TransFuser, a novel Multi-Modal Fusion Transformer, to integrate image and LiDAR representations using attention. We experimentally validate the efficacy of our approach in urban settings involving complex scenarios using the CARLA urban driving simulator. Our approach achieves state-of-the-art driving performance while reducing collisions by 76% compared to geometry-based fusion. </em>
<pre xml:space="preserve" id="prakash2021multimodalBib">

@inProceedings{prakash2021multimodal,
  title={Multi-Modal Fusion Transformer for End-to-End Autonomous Driving},
  author = {Aditya Prakash
    and Kashyap Chitta
    and Andreas Geiger},
  booktitle={Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021}
}</pre></td>
<script language="javascript" type="text/javascript" xml:space="preserve">
hideblock('prakash2021multimodalAbs');
hideblock('prakash2021multimodalBib');
</script>
</div>
</div>
<!--------------------------------------------------------------------------->

<!--------------------------------------------------------------------------->
<div class="paper" id="behl2020label">
<img class="paper" title="Label Efficient Visual Abstractions for Autonomous Driving" src="figures/behl2020label.png" />
<p> <b id="papertitle"><a href="https://arxiv.org/abs/2005.10091">Label Efficient Visual Abstractions for Autonomous Driving</a></b> <br/>
Aseem Behl*, <strong>Kashyap Chitta*</strong>, Aditya Prakash, Eshed Ohn-Bar, Andreas Geiger <br/>
International Conference on Intelligent Robots and Systems (IROS), 2020 <br/>
<a href="https://arxiv.org/pdf/2005.10091.pdf">pdf </a>  &nbsp <a href="javascript:toggleblock('behl2020labelAbs')">abstract </a>  &nbsp <a href="javascript:toggleblock('behl2020labelBib')">bibtex </a> &nbsp <a href="https://github.com/autonomousvision/visual_abstractions">code </a> &nbsp <a href="https://autonomousvision.github.io/visual-abstractions/">blog </a> </p>
<div class="papermeta" id="behl2020labelMeta">
<em id="behl2020labelAbs">It is well known that semantic segmentation can be used as an effective intermediate representation for learning driving policies. However, the task of street scene semantic segmentation requires expensive annotations. Furthermore, segmentation algorithms are often trained irrespective of the actual driving task, using auxiliary image-space loss functions which are not guaranteed to maximize driving metrics such as safety or distance traveled per intervention. In this work, we seek to quantify the impact of reducing segmentation annotation costs on learned behavior cloning agents. We analyze several segmentation-based intermediate representations. We use these visual abstractions to systematically study the trade-off between annotation efficiency and driving performance, i.e., the types of classes labeled, the number of image samples used to learn the visual abstraction model, and their granularity (e.g., object masks vs. 2D bounding boxes). Our analysis uncovers several practical insights into how segmentation-based visual abstractions can be exploited in a more label efficient manner. Surprisingly, we find that state-of-the-art driving performance can be achieved with orders of magnitude reduction in annotation cost. Beyond label efficiency, we find several additional training benefits when leveraging visual abstractions, such as a significant reduction in the variance of the learned policy when compared to state-of-the-art end-to-end driving models. </em>
<pre xml:space="preserve" id="behl2020labelBib">

@inProceedings{behl2020label,
  title={Label Efficient Visual Abstractions for Autonomous Driving},
  author = {Aseem Behl
    and Kashyap Chitta
    and Aditya Prakash
    and Eshed Ohn-Bar
    and Andreas Geiger},
  booktitle={International Conference on Intelligent Robots and Systems (IROS)},
  year={2020}
}</pre></td>
<script language="javascript" type="text/javascript" xml:space="preserve">
hideblock('behl2020labelAbs');
hideblock('behl2020labelBib');
</script>
</div>
</div>
<!--------------------------------------------------------------------------->

<!--------------------------------------------------------------------------->
<div class="paper" id="haussmann2020scalable">
<img class="paper" title="Scalable Active Learning for Object Detection" src="figures/haussmann2020scalable.png" />
<p> <b id="papertitle"><a href="https://arxiv.org/abs/2004.04699">Scalable Active Learning for Object Detection</a></b> <br/>
Elmar Haussmann, Michele Fenzi, <strong>Kashyap Chitta</strong>, Jan Ivanecky, Hanson Xu, Donna Roy, Akshita Mittel, Nicolas Koumchatzky, Clement Farabet, Jose M. Alvarez <br/>
IEEE Intelligent Vehicles Symposium (IV), 2020 <br/>
<a href="https://arxiv.org/pdf/2004.04699.pdf">pdf </a>  &nbsp <a href="javascript:toggleblock('haussmann2020scalableAbs')">abstract </a>  &nbsp <a href="javascript:toggleblock('haussmann2020scalableBib')">bibtex </a> &nbsp <a href="https://medium.com/nvidia-ai/scalable-active-learning-for-autonomous-driving-a-practical-implementation-and-a-b-test-4d315ed04b5f">blog </a> </p>
<div class="papermeta" id="haussmann2020scalableMeta">
<em id="haussmann2020scalableAbs">Deep Neural Networks trained in a fully supervised fashion are the dominant technology in perception-based autonomous driving systems. While collecting large amounts of unlabeled data is already a major undertaking, only a subset of it can be labeled by humans due to the effort needed for high-quality annotation. Therefore, finding the right data to label has become a key challenge. Active learning is a powerful technique to improve data efficiency for supervised learning methods, as it aims at selecting the smallest possible training set to reach a required performance. We have built a scalable production system for active learning in the domain of autonomous driving. In this paper, we describe the resulting high-level design, sketch some of the challenges and their solutions, present our current results at scale, and briefly describe the open problems and future directions. </em>
<pre xml:space="preserve" id="haussmann2020scalableBib">

@inProceedings{haussmann2020scalable,
  title={Scalable Active Learning for Object Detection},
  author = {Elmar Haussmann
    and Michele Fenzi
    and Kashyap Chitta
    and Jan Ivanecky
    and Hanson Xu
    and Donna Roy
    and Akshita Mittel
    and Nicolas Koumchatzky
    and Clement Farabet
    and Jose M. Alvarez},
  booktitle={IEEE Intelligent Vehicles Symposium (IV)},
  year={2020}
}</pre></td>
<script language="javascript" type="text/javascript" xml:space="preserve">
hideblock('haussmann2020scalableAbs');
hideblock('haussmann2020scalableBib');
</script>
</div>
</div>
<!--------------------------------------------------------------------------->

<!--------------------------------------------------------------------------->
<div class="paper" id="ohn-bar2020learning">
<img class="paper" title="Learning Situational Driving" src="figures/ohn-bar2020learning.jpg" />
<p> <b id="papertitle"><a href="http://openaccess.thecvf.com/content_CVPR_2020/html/Ohn-Bar_Learning_Situational_Driving_CVPR_2020_paper.html">Learning Situational Driving</a></b> <br/>
Eshed Ohn-Bar, Aditya Prakash, Aseem Behl, <strong>Kashyap Chitta</strong>, Andreas Geiger <br/>
Conference on Computer Vision and Pattern Recognition (CVPR), 2020 <br/>
<a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Ohn-Bar_Learning_Situational_Driving_CVPR_2020_paper.pdf">pdf </a>  &nbsp <a href="javascript:toggleblock('ohn-bar2020learningAbs')">abstract </a>  &nbsp <a href="javascript:toggleblock('ohn-bar2020learningBib')">bibtex </a> </p>
<div class="papermeta" id="ohn-bar2020learningMeta">
<em id="ohn-bar2020learningAbs">Human drivers have a remarkable ability to drive in diverse visual conditions and situations, e.g., from maneuvering in rainy, limited visibility conditions with no lane markings to turning in a busy intersection while yielding to pedestrians. In contrast, we find that state-of-the-art sensorimotor driving models struggle when encountering diverse settings with varying relationships between observation and action. To generalize when making decisions across diverse conditions, humans leverage multiple types of situation-specific reasoning and learning strategies. Motivated by this observation, we develop a framework for learning a situational driving policy that effectively captures reasoning under varying types of scenarios. Our key idea is to learn a mixture model with a set of policies that can capture multiple driving modes. We first optimize the mixture model through behavior cloning, and show it to result in significant gains in terms of driving performance in diverse conditions. We then refine the model by directly optimizing for the driving task itself, i.e., supervised with the navigation task reward. Our method is more scalable than methods assuming access to privileged information, e.g., perception labels, as it only assumes demonstration and reward-based supervision. We achieve over 98% success rate on the CARLA driving benchmark as well as state-of-the-art performance on a newly introduced generalization benchmark. </em>
<pre xml:space="preserve" id="ohn-bar2020learningBib">

@inProceedings{ohn-bar2020learning,
  title={Learning Situational Driving},
  author = {Eshed Ohn-Bar
    and Aditya Prakash
    and Aseem Behl
    and Kashyap Chitta
    and Andreas Geiger},
  booktitle={Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020}
}</pre></td>
<script language="javascript" type="text/javascript" xml:space="preserve">
hideblock('ohn-bar2020learningAbs');
hideblock('ohn-bar2020learningBib');
</script>
</div>
</div>
<!--------------------------------------------------------------------------->

<!--------------------------------------------------------------------------->
<div class="paper" id="prakash2020exploring">
<img class="paper" title="Exploring Data Aggregation in Policy Learning for Vision-Based Urban Autonomous Driving" src="figures/prakash2020exploring.jpg" />
<p> <b id="papertitle"><a href="http://openaccess.thecvf.com/content_CVPR_2020/html/Prakash_Exploring_Data_Aggregation_in_Policy_Learning_for_Vision-Based_Urban_Autonomous_CVPR_2020_paper.html">Exploring Data Aggregation in Policy Learning for Vision-Based Urban Autonomous Driving</a></b> <br/>
Aditya Prakash, Aseem Behl, Eshed Ohn-Bar, <strong>Kashyap Chitta</strong>, Andreas Geiger <br/>
Conference on Computer Vision and Pattern Recognition (CVPR), 2020 <br/>
<a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Prakash_Exploring_Data_Aggregation_in_Policy_Learning_for_Vision-Based_Urban_Autonomous_CVPR_2020_paper.pdf">pdf </a>  &nbsp <a href="javascript:toggleblock('prakash2020exploringAbs')">abstract </a>  &nbsp <a href="javascript:toggleblock('prakash2020exploringBib')">bibtex </a>  &nbsp <a href="https://github.com/autonomousvision/data_aggregation">code </a> &nbsp <a href="https://autonomousvision.github.io/dagger-urban-driving/">blog </a> </p>
<div class="papermeta" id="prakash2020exploringMeta">
<em id="prakash2020exploringAbs">Data aggregation techniques can significantly improve vision-based policy learning within a training environment, e.g., learning to drive in a specific simulation condition. However, as on-policy data is sequentially sampled and added in an iterative manner, the policy can specialize and overfit to the training conditions. For real-world applications, it is useful for the learned policy to generalize to novel scenarios that differ from the training conditions. To improve policy learning while maintaining robustness when training end-to-end driving policies, we perform an extensive analysis of data aggregation techniques in the CARLA environment. We demonstrate how the majority of them have poor generalization performance, and develop a novel approach with empirically better generalization performance compared to existing techniques. Our two key ideas are (1) to sample critical states from the collected on-policy data based on the utility they provide to the learned policy in terms of driving behavior, and (2) to incorporate a replay buffer which progressively focuses on the high uncertainty regions of the policy's state distribution. We evaluate the proposed approach on the CARLA NoCrash benchmark, focusing on the most challenging driving scenarios with dense pedestrian and vehicle traffic. Our approach improves driving success rate by 16% over state-of-the-art, achieving 87% of the expert performance while also reducing the collision rate by an order of magnitude without the use of any additional modality, auxiliary tasks, architectural modifications or reward from the environment. </em>
<pre xml:space="preserve" id="prakash2020exploringBib">

@inProceedings{prakash2020exploring,
  title={Exploring Data Aggregation in Policy Learning for Vision-Based Urban Autonomous Driving},
  author = {Aditya Prakash
    and Aseem Behl
    and Eshed Ohn-Bar
    and Kashyap Chitta
    and Andreas Geiger},
  booktitle={Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020}
}</pre></td>
<script language="javascript" type="text/javascript" xml:space="preserve">
hideblock('prakash2020exploringAbs');
hideblock('prakash2020exploringBib');
</script>
</div>
</div>
<!--------------------------------------------------------------------------->

<!--------------------------------------------------------------------------->
<div class="paper" id="chitta2020quadtree">
<img class="paper" title="Quadtree Generating Networks: Efficient Hierarchical Scene Parsing with Sparse Convolutions" src="figures/chitta2020quadtree.png" />
<p> <b id="papertitle"><a href="http://openaccess.thecvf.com/content_WACV_2020/html/Chitta_Quadtree_Generating_Networks_Efficient_Hierarchical_Scene_Parsing_with_Sparse_Convolutions_WACV_2020_paper.html">Quadtree Generating Networks: Efficient Hierarchical Scene Parsing with Sparse Convolutions</a></b> <br/>
<strong>Kashyap Chitta</strong>, Jose M. Alvarez, Martial Hebert <br/>
IEEE Winter Conference on Applications of Computer Vision (WACV), 2020 <br/>
<a href="http://openaccess.thecvf.com/content_WACV_2020/papers/Chitta_Quadtree_Generating_Networks_Efficient_Hierarchical_Scene_Parsing_with_Sparse_Convolutions_WACV_2020_paper.pdf">pdf </a>  &nbsp <a href="javascript:toggleblock('chitta2020quadtreeAbs')">abstract </a>  &nbsp <a href="javascript:toggleblock('chitta2020quadtreeBib')">bibtex </a>  &nbsp <a href="https://github.com/kashyap7x/QGN">code </a> </p>
<div class="papermeta" id="chitta2020quadtreeMeta">
<em id="chitta2020quadtreeAbs">Semantic segmentation with Convolutional Neural Networks is a memory-intensive task due to the high spatial resolution of feature maps and output predictions. In this paper, we present Quadtree Generating Networks (QGNs), a novel approach able to drastically reduce the memory footprint of modern semantic segmentation networks. The key idea is to use quadtrees to represent the predictions and target segmentation masks instead of dense pixel grids. Our quadtree representation enables hierarchical processing of an input image, with the most computationally demanding layers only being used at regions in the image containing boundaries between classes. In addition, given a trained model, our representation enables flexible inference schemes to trade-off accuracy and computational cost, allowing the network to adapt in constrained situations such as embedded devices. We demonstrate the benefits of our approach on the Cityscapes, SUN-RGBD and ADE20k datasets. On Cityscapes, we obtain an relative 3% mIoU improvement compared to a dilated network with similar memory consumption; and only receive a 3% relative mIoU drop compared to a large dilated network, while reducing memory consumption by over 4&times. </em>
<pre xml:space="preserve" id="chitta2020quadtreeBib">

@inProceedings{chitta2020quadtree,
  title={Quadtree Generating Networks: Efficient Hierarchical Scene Parsing with Sparse Convolutions},
  author = {Kashyap Chitta
    and Jose M. Alvarez
    and Martial Hebert},
  booktitle={IEEE Winter Conference on Applications of Computer Vision (WACV)},
  year={2020}
}</pre></td>
<script language="javascript" type="text/javascript" xml:space="preserve">
hideblock('chitta2020quadtreeAbs');
hideblock('chitta2020quadtreeBib');
</script>
</div>
</div>
<!--------------------------------------------------------------------------->

<!--------------------------------------------------------------------------->
<div class="paper" id="chitta2018deep">
<img class="paper" title="Deep Probabilistic Ensembles: Approximate Variational Inference through KL Regularization" src="figures/chitta2018deep.png" />
<p><b id="papertitle"><a href="https://arxiv.org/abs/1811.02640">Deep Probabilistic Ensembles: Approximate Variational Inference through KL Regularization</a></b> <br/>
<strong>Kashyap Chitta</strong>, Jose M. Alvarez, Adam Lesnikowski <br/>
Workshop on Bayesian Deep Learning (BDL), NeurIPS, 2018 <br/>
<a href="https://arxiv.org/pdf/1811.02640.pdf">pdf </a>  &nbsp <a href="javascript:toggleblock('chitta2018deepAbs')">abstract </a>  &nbsp <a href="javascript:toggleblock('chitta2018deepBib')">bibtex </a> </p>
<div class="papermeta" id="chitta2018deepMeta">
<em id="chitta2018deepAbs"> In this paper, we introduce Deep Probabilistic Ensembles (DPEs), a scalable technique that uses a regularized ensemble to approximate a deep Bayesian Neural Network (BNN). We do so by incorporating a KL divergence penalty term into the training objective of an ensemble, derived from the evidence lower bound used in variational inference. We evaluate the uncertainty estimates obtained from our models for active learning on visual classification. Our approach steadily improves upon active learning baselines as the annotation budget is increased.</em>
<pre xml:space="preserve" id="chitta2018deepBib">

@inProceedings{chitta2018deep,
  title={Deep Probabilistic Ensembles: Approximate Variational Inference through KL Regularization},
  author = {Kashyap Chitta
    and Jose M. Alvarez
    and Adam Lesnikowski},
  booktitle={Workshop on Bayesian Deep Learning (BDL), Conference on Neural Information Processing Systems (NeurIPS)},
  year={2018}
}</pre></td>
<script language="javascript" type="text/javascript" xml:space="preserve">
hideblock('chitta2018deepAbs');
hideblock('chitta2018deepBib');
</script>
</div>
</div>
<!--------------------------------------------------------------------------->

<!--------------------------------------------------------------------------->
<div class="paper" id="chitta2018targeted">
<img class="paper" title="Targeted Kernel Networks: Faster Convolutions with Attentive Regularization" src="figures/chitta2018targeted.png" />
<p><b id="papertitle"><a href="https://arxiv.org/abs/1806.00523">Targeted Kernel Networks: Faster Convolutions with Attentive Regularization</a></b> <br/>
<strong>Kashyap Chitta</strong><br/>
Workshop on Compact and Efficient Feature Representation and Learning in Computer Vision (CEFRL), ECCV, 2018 <br/>
<a href="https://arxiv.org/pdf/1806.00523.pdf">pdf </a>  &nbsp <a href="javascript:toggleblock('chitta2018targetedAbs')">abstract </a>  &nbsp <a href="javascript:toggleblock('chitta2018targetedBib')">bibtex </a>  &nbsp <a href="https://github.com/kashyap7x/Attentive-Regularization">code </a> </p>
<div class="papermeta" id="chitta2018targetedMeta">
<em id="chitta2018targetedAbs"> We propose Attentive Regularization (AR), a method to constrain the activation maps of kernels in Convolutional Neural Networks (CNNs) to specific regions of interest (ROIs). Each kernel learns a location of specialization along with its weights through standard backpropagation. A differentiable attention mechanism requiring no additional supervision is used to optimize the ROIs. Traditional CNNs of different types and structures can be modified with this idea into equivalent Targeted Kernel Networks (TKNs), while keeping the network size nearly identical. By restricting kernel ROIs, we reduce the number of sliding convolutional operations performed throughout the network in its forward pass, speeding up both training and inference. We evaluate our proposed architecture on both synthetic and natural tasks across multiple domains. TKNs obtain significant improvements over baselines, requiring less computation (around an order of magnitude) while achieving superior performance.</em>
<pre xml:space="preserve" id="chitta2018targetedBib">

@inProceedings{chitta2018targeted,
  title={Targeted Kernel Networks: Faster Convolutions with Attentive Regularization},
  author = {Kashyap Chitta},
  booktitle={Workshop on Compact and Efficient Feature Representation and Learning in Computer Vision (CEFRL), European Conference on Computer Vision (ECCV)},
  year={2018}
}</pre></td>
<script language="javascript" type="text/javascript" xml:space="preserve">
hideblock('chitta2018targetedAbs');
hideblock('chitta2018targetedBib');
</script>
</div>
</div>
<!--------------------------------------------------------------------------->

<!--------------------------------------------------------------------------->
<div class="paper" id="chitta2016reduced">
<img class="paper" title="A Reduced Region of Interest Based Approach for Facial Expression Recognition from Static Images" src="figures/chitta2016reduced.png" />
<p><b id="papertitle"><a href="https://ieeexplore.ieee.org/document/7848553">A Reduced Region of Interest Based Approach for Facial Expression Recognition from Static Images</a> </b> <br/>
<strong>Kashyap Chitta</strong>, Neeraj N. Sajjan <br/>
IEEE Region-10 Conference (TENCON), 2016 <br/>
<a href="javascript:toggleblock('chitta2016reducedAbs')">abstract </a>  &nbsp <a href="javascript:toggleblock('chitta2016reducedBib')">bibtex </a> </p>
<div class="papermeta" id="chitta2016reducedMeta">
<em id="chitta2016reducedAbs">The general approach to facial expression recognition involves three stages: face acquisition, feature extraction and expression recognition. A series of steps are used during feature extraction, and the robustness of a recognition model depends on the ability to handle exceptions over all these steps. This paper details experiments conducted to classify images by facial expression using reduced regions of interest and discriminative salient patches on the face, while minimizing the number of steps required for their localization. The performance of various feature descriptors is analyzed and a model for expression recognition for which experiments on the JAFFE database show effectiveness is proposed.</em>
<pre xml:space="preserve" id="chitta2016reducedBib">

@inProceedings{chitta2016reduced,
  title={A Reduced Region of Interest Based Approach for Facial Expression Recognition from Static Images},
  author = {Kashyap Chitta
    and Neeraj N. Sajjan},
  booktitle={IEEE Region-10 Conference (TENCON)},
  year={2016}
}</pre></td>
<script language="javascript" type="text/javascript" xml:space="preserve">
hideblock('chitta2016reducedAbs');
hideblock('chitta2016reducedBib');
</script>
</div>
</div>

<!-- 
<div class="section">
<h2> Preprints </h2>

<div class="paper" id="chitta2019training">
<img class="paper" title="Training Data Subset Search with Ensemble Active Learning" src="figures/chitta2019training.png" />
<p> <b id="papertitle"><a href="https://arxiv.org/abs/1905.12737">Training Data Subset Search with Ensemble Active Learning</a></b> <br/>
<strong>Kashyap Chitta</strong>, Jose M. Alvarez, Elmar Haussmann, Clement Farabet <br/>
ArXiv e-prints, 2019 <br/>
<a href="https://arxiv.org/pdf/1905.12737.pdf">pdf </a>  &nbsp <a href="javascript:toggleblock('chitta2019trainingAbs')">abstract </a>  &nbsp <a href="javascript:toggleblock('chitta2019trainingBib')">bibtex </a> </p>
<div class="papermeta" id="chitta2019trainingMeta">
<em id="chitta2019trainingAbs">Deep Neural Networks (DNNs) often rely on very large datasets for training. Given the large size of such datasets, it is conceivable that they contain certain samples that either do not contribute or negatively impact the DNN's optimization. Modifying the training distribution in a way that excludes such samples could provide an effective solution to both improve performance and reduce training time. In this paper, we propose to scale up ensemble Active Learning (AL) methods to perform acquisition at a large scale (10k to 500k samples at a time). We do this with ensembles of hundreds of models, obtained at a minimal computational cost by reusing intermediate training checkpoints. This allows us to automatically and efficiently perform a training data subset search for large labeled datasets. We observe that our approach obtains favorable subsets of training data, which can be used to train more accurate DNNs than training with the entire dataset. We perform an extensive experimental study of this phenomenon on three image classification benchmarks (CIFAR-10, CIFAR-100 and ImageNet), as well as an internal object detection benchmark for prototyping perception models for autonomous driving. Unlike existing studies, our experiments on object detection are at the scale required for production-ready autonomous driving systems. We provide insights on the impact of different initialization schemes, acquisition functions and ensemble configurations at this scale. Our results provide strong empirical evidence that optimizing the training data distribution can provide significant benefits on large scale vision tasks. </em>
<pre xml:space="preserve" id="chitta2019trainingBib">

@inProceedings{chitta2019training,
  title={Training Data Distribution Search with Ensemble Active Learning},
  author = {Kashyap Chitta
    and Jose M. Alvarez
    and Elmar Haussmann
    and Clement Farabet},
  booktitle={ArXiv e-prints},
  year={2019}
}</pre></td>
<script language="javascript" type="text/javascript" xml:space="preserve">
hideblock('chitta2019trainingAbs');
hideblock('chitta2019trainingBib');
</script>
</div>
</div>

<div class="paper" id="chitta2018largescale">
<img class="paper" title="Large-Scale Visual Active Learning with Deep Probabilistic Ensembles" src="figures/chitta2018largescale.png" />
<p> <b id="papertitle"><a href="https://arxiv.org/abs/1811.03575">Large-Scale Visual Active Learning with Deep Probabilistic Ensembles</a></b> <br/>
<strong>Kashyap Chitta</strong>, Jose M. Alvarez, Adam Lesnikowski <br/>
ArXiv e-prints, 2018 <br/>
<a href="https://arxiv.org/pdf/1811.03575.pdf">pdf </a>  &nbsp <a href="javascript:toggleblock('chitta2018largescaleAbs')">abstract </a>  &nbsp <a href="javascript:toggleblock('chitta2018largescaleBib')">bibtex </a> </p>
<div class="papermeta" id="chitta2018largescaleMeta">
<em id="chitta2018largescaleAbs">Annotating the right data for training deep neural networks is an important challenge. Active learning using uncertainty estimates from Bayesian Neural Networks (BNNs) could provide an effective solution to this. Despite being theoretically principled, BNNs require approximations to be applied to large-scale problems, where both performance and uncertainty estimation are crucial. In this paper, we introduce Deep Probabilistic Ensembles (DPEs), a scalable technique that uses a regularized ensemble to approximate a deep BNN. We conduct a series of large-scale visual active learning experiments to evaluate DPEs on classification with the CIFAR-10, CIFAR-100 and ImageNet datasets, and semantic segmentation with the BDD100k dataset. Our models require significantly less training data to achieve competitive performances, and steadily improve upon strong active learning baselines as the annotation budget is increased. </em>
<pre xml:space="preserve" id="chitta2018largescaleBib">

@inProceedings{chitta2018largescale,
  title={Large-Scale Visual Active Learning with Deep Probabilistic Ensembles},
  author = {Kashyap Chitta
    and Jose M. Alvarez
    and Adam Lesnikowski},
  booktitle={ArXiv e-prints},
  year={2018}
}</pre></td>
<script language="javascript" type="text/javascript" xml:space="preserve">
hideblock('chitta2018largescaleAbs');
hideblock('chitta2018largescaleBib');
</script>
</div>
</div>

<div class="paper" id="chitta2018adaptive">
<img class="paper" title="Adaptive Semantic Segmentation with a Strategic Curriculum of Proxy Labels" src="figures/chitta2018adaptive.png" />
<p> <b id="papertitle"><a href="https://arxiv.org/abs/1811.03542">Adaptive Semantic Segmentation with a Strategic Curriculum of Proxy Labels</a></b> <br/>
<strong>Kashyap Chitta</strong>, Jianwei Feng, Martial Hebert <br/>
ArXiv e-prints, 2018 <br/>
<a href="https://arxiv.org/pdf/1811.03542.pdf">pdf </a>  &nbsp <a href="javascript:toggleblock('chitta2018adaptiveAbs')">abstract </a>  &nbsp <a href="javascript:toggleblock('chitta2018adaptiveBib')">bibtex </a>  &nbsp <a href="https://github.com/kashyap7x/Domain-Adapatation">code </a> </p>
<div class="papermeta" id="chitta2018adaptiveMeta">
<em id="chitta2018adaptiveAbs">Training deep networks for semantic segmentation requires annotation of large amounts of data, which can be time-consuming and expensive. Unfortunately, these trained networks still generalize poorly when tested in domains not consistent with the training data. In this paper, we show that by carefully presenting a mixture of labeled source domain and proxy-labeled target domain data to a network, we can achieve state-of-the-art unsupervised domain adaptation results. With our design, the network progressively learns features specific to the target domain using annotation from only the source domain. We generate proxy labels for the target domain using the network's own predictions. Our architecture then allows selective mining of easy samples from this set of proxy labels, and hard samples from the annotated source domain. We conduct a series of experiments with the GTA5, Cityscapes and BDD100k datasets on synthetic-to-real domain adaptation and geographic domain adaptation, showing the advantages of our method over baselines and existing approaches.</em>
<pre xml:space="preserve" id="chitta2018adaptiveBib">

@inProceedings{chitta2018adaptive,
  title={Adaptive Semantic Segmentation with a Strategic Curriculum of Proxy Labels},
  author = {Kashyap Chitta
    and Jianwei Feng
    and Martial Hebert},
  booktitle={ArXiv e-prints},
  year={2018}
}</pre></td>
<script language="javascript" type="text/javascript" xml:space="preserve">
hideblock('chitta2018adaptiveAbs');
hideblock('chitta2018adaptiveBib');
</script>
</div>
</div>

<div class="paper" id="patel2018learning">
<img class="paper" title="Learning Sampling Policies for Domain Adaptation" src="figures/patel2018learning.png" />
<p><b id="papertitle"><a href="https://arxiv.org/abs/1805.07641">Learning Sampling Policies for Domain Adaptation</a></b> <br/>
Yash Patel*, <strong>Kashyap Chitta</strong>*, Bhavan Jasani* <br/>
ArXiv e-prints, 2018 <br/>
<a href="https://arxiv.org/pdf/1805.07641.pdf">pdf </a>  &nbsp <a href="javascript:toggleblock('patel2018learningAbs')">abstract </a>  &nbsp <a href="javascript:toggleblock('patel2018learningBib')">bibtex </a>  &nbsp <a href="https://github.com/yash0307/LearningSamplingPolicies">code </a> </p>
<div class="papermeta" id="patel2018learningMeta">
<em id="patel2018learningAbs">We address the problem of semi-supervised domain adaptation of classification algorithms through deep Q-learning. The core idea is to consider the predictions of a source domain network on target domain data as noisy labels, and learn a policy to sample from this data so as to maximize classification accuracy on a small annotated reward partition of the target domain. Our experiments show that learned sampling policies construct labeled sets that improve accuracies of visual classifiers over baselines.</em>
<pre xml:space="preserve" id="patel2018learningBib">

@inProceedings{patel2018learning,
  title={Learning Sampling Policies for Domain Adaptation},
  author = {Yash Patel
    and Kashyap Chitta
    and Bhavan Jasani},
  booktitle={ArXiv e-prints},
  year={2018}
}</pre></td>
<script language="javascript" type="text/javascript" xml:space="preserve">
hideblock('patel2018learningAbs');
hideblock('patel2018learningBib');
</script>
</div>
</div>
----------------------------------------------------------------------- -->

</body>
</html>
